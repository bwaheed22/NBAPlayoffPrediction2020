---
title: "Who Would Have Made the 2020 NBA Playoffs?"
subtitle: "Predicting NBA Playoff Teams with Machine Learning"
output: 
  pdf_document: 
    number_sections: yes
urlcolor: blue
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
# Load libraries and functions:
library(tidyverse)
library(caret)
library(rsample)
library(ROCR)
library(randomForest)
library(here)
```

```{r message=FALSE, warning=FALSE, include=FALSE}
set.seed(1001)
source("/Users/mbp/Documents/Side Projects/NBAPlayoffPrediction2020/Scripts/scrape_script.R")
nbadata <- read.csv("/Users/mbp/Documents/Side Projects/NBAPlayoffPrediction2020/Data/NBAdata.csv")
```

# Overview

In this analysis, I wanted to predict which NBA teams were going to make the playoffs in the 2019-2020 NBA season that was abruptly cut short due to the COVID-19 pandemic. Fans across the country were bummed and looking forward to a competitive, high-octane playoff season. The two top-seeded teams, the Los Angeles Lakers backed by LeBron James, and the Milwaukee Bucks lead by Giannis Antetokounmpo, were slated to duel for the championship title. 

At the time of cancellation (March 11, 2020), a few teams clinched their playoff berths, others were jostling for their seeding positions, and some were fighting for the coveted 8th seed. Usually, playoffs begin in the middle of April, and the top 8 teams of each conference (Eastern and Western) are eligible. I wanted to know which teams were going to make the playoffs, had the season continued. 

I used several classification machine learning models that predicted playoff teams based on aggregate team statistics over the course of the regular season. I trained the models on data from 15 NBA seasons (2004-2005 through 2018-2019) and predicted outcomes for the 2019-2020 season.

## Results

The final model predicted 17 teams to be in the 2020 NBA Playoffs (10 teams from the West, 7 in the East). 

* The model predicted two extra teams to qualify out of the West, who were not in the top 8 at the time.
* The model failed to predict one team to qualify out of the East, which was in the top 8 at the time. 
* Out of all the predicted playoff teams, 15 of them were in the top 8 seeds in their respective conferences at the time of cancellation.

# Data Collection

The data were scraped^[The data set and script for scraping and cleaning data can be found in the "Data" and "Scripts" folder, respectively on the [GitHub repository](https://github.com/bwaheed22/NBAPlayoffPrediction2020) and cleaned from the [ESPN website](https://www.espn.com/nba/stats/team) and consist of information for 30 teams over 15 seasons. The data contain traditional basketball statistics (aggregated on a per-game basis), and an indicator for whether or not that team made the playoffs in that season. The goal here is to find if any of these variables are indicative of a team’s chances of being a playoff team. 

Below are a sample of some of the metrics in the data and the first few records of the dataset:

* `"PTS" – Total points scored.`
* `"FGA" – Total field goal attempts (shots).`
* `"X3P" – Total three-point attempts.`
* `"FTM" – Total free-throws made.`
* `"REB" – Total rebounds collected.`
* `"AST" – Total assists.`
* `"STL" – Total steals.`
* `"TO" – Total turnovers (loss of posession of the ball).`
* `"PF" – Total personal fouls committed.`
* `"playoffs" – Indicator for playoffs (1 = in playoffs).`

\pagebreak

```{r echo=FALSE, message=FALSE, warning=FALSE}
kableExtra::kable(nbadata[1:5,c(2:5, 17:22)], format = "markdown")
```
# Data Exploration

## Correlation Plot

First let’s explore any variables significantly related to the outcome variable. The correlation plot below shows that most of variables are strongly correlated with each other, and have moderate correlations to the target variable, `playoffs`.

```{r echo=FALSE, warning=FALSE, fig.align = "center"}
corrplot::corrplot(cor(nbadata[,c(3:20,22)]))
```
\pagebreak

# Modeling the Data

Since the goal of this analysis is to classify a team as a playoff team or not, I decided to use classification models that specialize in predicting categorical (binary) outcomes. The data was split into a training and test set (70%/30% split) and the models were fit on the training data using repeated 10-fold cross-validation.

## Variables Included in the Model

The following variables were selected to be included in the models based on stepwise-AIC logistic regression:

`PTS, FGM , X3PA, FTM ,FTA, FT, OR, DR, AST, STL, TO`

The models and their respective performances on the training data are shown below:

```{r message=FALSE, warning=FALSE, include=FALSE}
# set train and test sets:
inTrain <- createDataPartition(y = nbadata$playoffs, p = 0.70, list = FALSE)
nbadata_train <- nbadata[inTrain,]
nbadata_test <- nbadata[-inTrain,]

# Re-code outcome variable as a factor:
nbadata_train$playoffs <- factor(nbadata_train$playoffs, labels = c("0","1"))
nbadata_test$playoffs <- factor(nbadata_test$playoffs, labels = c("0","1"))

# construct logistic regression model:
logit1 <- train(playoffs ~ PTS + FGM + FG + X3PA + FTM + FTA + FT + OR + DR + AST + STL + TO, 
                data = nbadata_train, 
                method = "glm", family = "binomial", 
                trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10))

# construct random forest model:
rf_model <- train(playoffs ~ PTS + FGM + FG + X3PA + FTM + FTA + FT + OR + DR + AST + STL + TO,
                  data = nbadata_train, method = 'rf', 
                  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10),
                  importance = TRUE, ntree = 300)

# construct naive bayes model:
nb_model <- train(playoffs ~ PTS + FGM + FG + X3PA + FTM + FTA + FT + OR + DR + AST + STL + TO,
                           trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10),
                           method = "nb",
                           data = nbadata_train)
# construct svm model:
svm_model <- train(playoffs ~ PTS + FGM + FG + X3PA + FTM + FTA + FT + OR + DR + AST + STL + TO,
                  trControl = trainControl(method = "repeatedcv", number = 10, repeats = 10),
                  method = "svmLinear",
                  data = nbadata_train)

# accuracies of each model:
accuracy_df <- data.frame(Model = c("Logistic Regression", "Random Forest", 
"Naive Bayes", "SVM Linear"), Accuracy = c(logit1$results[[2]], rf_model$results[[2]][1],
                                           nb_model$results[[4]][1], svm_model$results[[2]][1]))
```

```{r echo=FALSE, message=FALSE, warning=FALSE}
kableExtra::kable(accuracy_df, format = "markdown")
```

## Brief aside on the Logistic Regression Model

In the Logistic Regression, we model the Bernoulli data-generating process of the outcome variable `“playoffs”` $P(Playoffs = 1) = p$ by assuming a linear relationship between predictor variables and the log-odds of the event that $P(Playoffs = 1)$. 

This model takes the form:
$$log(\frac{p}{1-p}) = \beta_0 + \sum_{i=1}^n \beta_iX_i$$
$$\mbox{where } p = \mbox{probability of being in playoffs and } X_i = \mbox{predictor } i$$
Below, the output of the model shows that all predictors are statistically significant (p < 0.05). It is interesting to note that the predictors `STLS` and `TO` have coefficients of `2.148` and `-1.597`, respectively. This means:

* On average, a higher amount of turnovers translate to a smaller log-odds (and subsequently probability) of being in the playoffs, holding all other variables constant.

* Similarly, if a team has a high amount of steals, the probability is much greater.

This seems to confirm the idea posited by most basketball gurus that defense is the best offense, and that sticking to fundamentals of the game most often wins championships.

```{r echo=FALSE, message=FALSE, warning=FALSE}
kableExtra::kable(round(data.frame("Coefficient" = coef(summary(logit1))[,1], "P-value" = coef(summary(logit1))[,4]),3), format = "markdown")
```

\pagebreak
# Model Evaluation

## Confusion Matricies

Now that the models are trained on the training data, we can evaluate their performance on the test sets and see how well each can distinguish between a playoff team and a non-playoff team.

> **For this analysis, I wanted to choose a model that is able to detect a playoff team well, but also limits the amount of playoff teams that it misses (i.e a balance between false positives and false negatives).**

Based on the confusion matrix plots below, the SVM model appears to be the best at predicting out-of-sample data, since it has the lowest False Positive Rate (FPR) and False Negative Rates (FNR) (rows represent true values, and columns represent the predicted values).

```{r message=FALSE, warning=FALSE, include=FALSE}
preds_logit<- predict(logit1, newdata = nbadata_test, type = 'raw')
preds_rf <- predict(rf_model, newdata= nbadata_test)
preds_nb <- predict(nb_model, newdata = nbadata_test)
preds_svm <- predict(svm_model, newdata = nbadata_test)
```

```{r echo=FALSE, fig.align="center", message=FALSE, warning=FALSE}
par(mfrow = c(2,2))
fourfoldplot(confusionMatrix(factor(nbadata_test$playoffs), 
                             factor(preds_logit), 
                             dnn = c("True", "Predicted"))$table, main = "Logistic Model")
fourfoldplot(confusionMatrix(factor(nbadata_test$playoffs), 
                              factor(preds_rf), 
                              dnn = c("True", "Predicted"))$table, main = "Random Forest Model")
fourfoldplot(confusionMatrix(factor(nbadata_test$playoffs), 
                             factor(preds_nb), 
                             dnn = c("True", "Predicted"))$table, main = "Naive Bayes Model")
fourfoldplot(confusionMatrix(factor(nbadata_test$playoffs), 
                             factor(preds_svm), 
                             dnn = c("True", "Predicted"))$table, main = "SVM Linear Model")

```

\pagebreak

## ROC and AUC

To confirm the selection of the SVM model, we can also look at the ROC curve and the associated AUC metric (area under the curve).

```{r echo=FALSE, message=FALSE, warning=FALSE}
roc_svm <- performance(prediction(as.numeric(preds_svm), as.numeric(nbadata_test$playoffs)), "tpr", "fpr")
auc_svm <- performance(prediction(as.numeric(preds_svm), as.numeric(nbadata_test$playoffs)), "auc")@y.values

plot(roc_svm, main = "ROC - SVM Linear Model", sub = "Area Under Curve (AUC) = 0.827")
abline(a = 0, b = 1, col = "red")
```

The AUC's for the other models are shown below:

```{r echo=FALSE, message=FALSE, warning=FALSE}
auc_logit <- performance(prediction(as.numeric(preds_logit), as.numeric(nbadata_test$playoffs)), "auc")@y.values
auc_rf <- performance(prediction(as.numeric(preds_rf), as.numeric(nbadata_test$playoffs)), "auc")@y.values
auc_nb<- performance(prediction(as.numeric(preds_nb), as.numeric(nbadata_test$playoffs)), "auc")@y.values

kableExtra::kable(cbind(c("SVM Linear Model", "Logistic Model", "Random Forest Model", "Naive Bayes Model"), round(rbind(unlist(auc_svm),unlist(auc_logit), unlist(auc_rf), unlist(auc_nb)),2)), format = "markdown", 
                  col.names = c("Model", "AUC Metric"))
```

\pagebreak
# Predicting 2020 NBA Playoff Teams 

According to the metrics, the SVM model performed best at classifying playoff teams. Let’s see how it performs on predicting the 2020 NBA season.

The predictions are shown below along with the teams that were among the top 16 in the league at the time the season was cancelled:

```{r echo=FALSE, message=FALSE, warning=FALSE}
nba2020 <- getdata(2020,2020)
nba2020$preds_svm <- predict(svm_model, newdata = nba2020)
actual_teams <-data.frame(teams = c("Milwaukee Bucks","Houston Rockets","Dallas Mavericks","LA Clippers",
                               "Los Angeles Lakers", "Brooklyn Nets","Boston Celtics","Toronto Raptors",
                               "Memphis Grizzlies","Miami Heat", "Utah Jazz", "Oklahoma City Thunder",
                               "Denver Nuggets", "Philadelphia 76ers", "Indiana Pacers", "Orlando Magic", "-"))
kableExtra::kable(data.frame(c(1:17), nba2020 %>% filter(preds_svm==1) %>% select(teams), actual_teams),
                        format = "markdown", col.names = c("", "Predicted Playoff Teams", "Actual Top 16 Teams"))
```

This model performed pretty well, as it predicted that most of the teams that were in the top 16 in the league would eventually make it to the playoffs. This seems reasonable because usually around March, teams begin to solidify their playoff berths, and the teams who are top 8 in their respective conferences at that time are the ones that will be in the playoffs. There are obvious exceptions, for instance, a team could go on a significant winning or losing streak, or, there is fierce competition and only a few games separate the last few seeds of the conference.

* The model predicted two teams in the West as playoff teams (San Antonio Spurs and Phoenix Suns) that were not in the top 8 of the West at the time.

* The model also failed to classify the Brooklyn Nets as a playoff team in the East, even though it was in the top 8 at the time.

\pagebreak

Let's examine the average statistics of these predicted playoff teams and compare them to the non-playoff teams:

```{r echo=FALSE, message=FALSE, warning=FALSE}
preds_avg_po <- nba2020 %>% filter(preds_svm == 1) %>% 
  select(PTS, FGM , X3PA, FTM ,FTA, FT, OR, DR, AST, STL, TO) %>% summarise(Average = colMeans(.))

preds_avg_npo <- nba2020 %>% filter (preds_svm == 0) %>% 
  select(PTS, FGM , X3PA, FTM ,FTA, FT, OR, DR, AST, STL, TO) %>% summarise(Average = colMeans(.))

varnames <- names(nba2020 %>% select(PTS, FGM , X3PA, FTM ,FTA, FT, OR, DR, AST, STL, TO))

kableExtra::kable(cbind(varnames, preds_avg_po, preds_avg_npo, preds_avg_po - preds_avg_npo), format = "markdown", col.names = c("Variable", "Playoff Teams", "Non-Playoff Teams", "Difference (Playoff v. Non-Playoff)"), digits = 3)
```

On average, it seems that there are clear differences between playoff and non-playoff teams, namely in PTS, FGM, ASTS, DR. 

We can conduct a difference in means T-test for each variable to see if these differences are statistically significant, and verify which aspects of the game that playoff teams generally perform better at.

```{r echo=FALSE}
po_stats <- nba2020 %>% filter(preds_svm == 1) %>% 
  select(PTS, FGM , X3PA, FTM ,FTA, FT, OR, DR, AST, STL, TO)

npo_stats <- nba2020 %>% filter(preds_svm == 0) %>% 
  select(PTS, FGM , X3PA, FTM ,FTA, FT, OR, DR, AST, STL, TO)

t_pvalue <- c()
for (i in 1:ncol(po_stats)) {
  t_pvalue[i] <- t.test(po_stats[,i], npo_stats[,i], var.equal = T)$p.value
}

kableExtra::kable(cbind(varnames, round(t_pvalue,3)),
                  format = "markdown", 
                  col.names = c("Variable", "T-Test P-Value"))
```

The results show that on average, playoff teams have statistically significant differences in the following metrics (at a significance level 0.05):

`PTS, FGM, FT, OR, DR, TO`

This means that generally, teams who score more points, make more field goals, and handle the ball well are the teams ending up in the playoffs.

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
nba2020$pred_logit<- predict(logit1, newdata = nba2020, type = 'raw')
nba2020$preds_rf <- predict(rf_model, newdata= nba2020)
nba2020$preds_nb <- predict(nb_model, newdata = nba2020)

summary2020 <- gdata::cbindX(matrix(c(1:29),ncol=1),nba2020 %>% filter(pred_logit==1) %>% select(teams),
nba2020 %>% filter(preds_rf==1) %>% select(teams),
nba2020 %>% filter(preds_nb==1) %>% select(teams))

names(summary2020) = c("", "Logistic Model", "Random Forest Model", "Naive Bayes Model")
```

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
kableExtra::kable(summary2020, format = "markdown")
```

# Key Takeaways

This analysis showed that traditional basketball statistics are useful in predicting NBA playoff eligibility. It provided insight into specific factors of a team that can lead to better performance.

The results suggest that if teams understand their metrics early in the season, they can focus on improving in specific areas of their play style (for example, limiting turnovers, or ensuring excellent shot selection and rebounding) to better their chances of qualifying for the playoffs. This also allows coaches and basketball management to make better informed decisions in terms of trades, and coaching styles. 